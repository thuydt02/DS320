{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef8173ac-c67c-41c7-b81f-08cce1e3c20d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a22382bdb8e4ba99",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "\n",
    "# DS320 Spring 2023: Midterm Project\n",
    "\n",
    "Due on Mon 04/03/23 at 8:00 AM\n",
    "\n",
    "The focus of this assignment is data <i><b>cleaning, preparation, and basis calculations</i></b>: deal with numbers, datetime values, and text\n",
    "\n",
    "You should review the pipeline of cleaning and preparing data I drew on the whiteboard and the \"data cleaning\" slide (try to be clear about 05 steps there) before working on this assignment.\n",
    "\n",
    "You will clean tweets taken from https://github.com/thuydt02/HCQ for a text sentiment analysis problem.\n",
    "You need to download the dataset from this link, upzip it and work on the `Full_Tweet_to_github.csv` file.\n",
    "Do not upload the dataset to your Jupyter Luther since it is big. You need to download this notebook and work on it with Google Colab. I am trying to find out a way to upload the dataset to Jupyter Luther. If it is successful (likely), I will let you know. But be prepared to work in Google Colab.  \n",
    "\n",
    "The dataset has roughly 164K tweets. These tweets were pulled from Twitter, satisfies:\n",
    "\n",
    "1. created in the year 2020\n",
    "2. has the key word \"Hydroxychroloquine\"\n",
    "\n",
    "I use this dataset for my research. I want to analyze the reactions and opinions of social network users on using the medication \"Hydroxychloroqine\" to treat COVID-19 disease.\n",
    "\n",
    "See more about the paper: https://arxiv.org/pdf/2201.00237.pdf\n",
    "\n",
    "There are 10 tasks, each is worth 10 points.\n",
    "\n",
    "Note: I will mannually grade your code, so no test cases will be provided, but I can give you the expectation of the outcomes for tasks as I can.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ad12b9a-4724-4094-aee8-a50db05d5dca",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-67cb81fd0b830d22",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/LC/doth02/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#setting up: You have to run this code cell first to compile helping functions.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "#from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "#functions for pre processing \n",
    "\n",
    "#---clean up html elements and entities: e.g. <html> </html> &nbsp;\n",
    "def cleanHtml(sentence):\n",
    "    #cleanr = re.compile('<.*?>')\n",
    "    cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    cleantext = re.sub(cleanr, ' ', str(sentence))\n",
    "    return cleantext\n",
    "\n",
    "\n",
    "#---function to clean the word of any punctuation or special characters\n",
    "def cleanPunc(sentence): \n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r' ',sentence)\n",
    "    cleaned = re.sub(r'[.|,|:|;|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def keepAlpha(sentence):\n",
    "    alpha_sent = \"\"\n",
    "    for word in sentence.split():\n",
    "        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n",
    "        alpha_sent += alpha_word\n",
    "        alpha_sent += \" \"\n",
    "    alpha_sent = alpha_sent.strip()\n",
    "    return alpha_sent\n",
    "\n",
    "#---stop words removing \n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['  ', 'zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within'])\n",
    "re_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\n",
    "\n",
    "def removeStopWords(sentence):\n",
    "    global re_stop_words\n",
    "    return re_stop_words.sub(\" \", sentence)\n",
    "\n",
    "\n",
    "#--- sentence stemering\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff3d80a-fc80-4526-9bd3-e7884d9b5e87",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1a1898bff464600a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Task 01: Understand the data set\n",
    "- Read the whole tweet dataset (Full_Tweet_to_github.csv file) into a dataframe. Name it `df_tweets`.\n",
    "From now, you will work on `df_tweets`.\n",
    "- Find out information about the follows. You must show your code:\n",
    "    Shape\n",
    "\n",
    "    Data types of all columns\n",
    "\n",
    "    Numerical columns\n",
    "\n",
    "    Text columns\n",
    "\n",
    "    Categorical columns\n",
    "\n",
    "    Date/time columns\n",
    "\n",
    "    Statistics (min, max, mean, std, ...) of all the numerical columns\n",
    "    \n",
    " Expectation: df_tweets.shape = (164168, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c2e1521-0fac-4375-b40d-0867cf639553",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-013181b5001bcc34",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/Full_Tweet_to_github.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m###BEGIN SOLUTION\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_tweets \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/Full_Tweet_to_github.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m      3\u001b[0m df_original_tweets \u001b[38;5;241m=\u001b[39m df_tweets\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_tweets\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/Full_Tweet_to_github.csv'"
     ]
    }
   ],
   "source": [
    "###BEGIN SOLUTION\n",
    "df_tweets = pd.read_csv(\"../data/Full_Tweet_to_github.csv\") \n",
    "df_original_tweets = df_tweets.copy()\n",
    "print(df_tweets.shape)\n",
    "print(df_tweets.dtypes)\n",
    "#df_tweets.describe()\n",
    "###END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7896405f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-073f5375cf68d705",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Task 02: Delete uneccessary columns\n",
    "As you can see, the following columns will not help a machine learning algorithm to learn:\n",
    "+ HYDROXYCHLOROQUINE: all the values in this columns are 1\n",
    "+ query_string: the URL link to the tweet in Twitter.\n",
    "\n",
    "Delete two above columns from `df_tweets`\n",
    "\n",
    "expectaion: the shape of the returned dataframe = (164168, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2638f6a6",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-bf8152b9e3074cb2",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "###BEGIN SOLUTION\n",
    "df_tweets.drop(axis = 'columns', columns = ['HYDROXYCHLOROQUINE','query_string'], inplace = True)\n",
    "print(df_tweets.shape)\n",
    "df_tweets.head()\n",
    "###END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3ed468",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c6e43af6ea21e74c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Task 03: Delete columns with missing values.\n",
    "\n",
    "Delete all columns with more than 80% missing values.\n",
    "\n",
    "In the previous assignment, you were asked to create a function, `delete_cols()`, call that function on `df_tweets` and `80`.\n",
    "\n",
    "Expectation: df_tweets.shape = (164168, 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca94bd8",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ddd75c64811f4103",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "###BEGIN SOLUTION\n",
    "def get_missing_percent_in_cols(df):\n",
    "    percent_missing = df.isnull().sum() * 100 / df.shape[0]\n",
    "    df_cols = pd.DataFrame(data = {'column_name': df.columns,\n",
    "                                   'percent_missing': percent_missing,\n",
    "                                   'data_type': df.dtypes})\n",
    "\n",
    "    df_cols.sort_values(['percent_missing'], ascending = False, inplace = True)\n",
    "    return df_cols\n",
    "\n",
    "\n",
    "def delete_cols(df, threshold):\n",
    "    df_cols = get_missing_percent_in_cols(df)\n",
    "    col_to_del = df_cols[df_cols['percent_missing'] > threshold]['column_name']\n",
    "    return df.drop(col_to_del, axis='columns', inplace = False)\n",
    "\n",
    "\n",
    "#usage\n",
    "df_tweets = delete_cols(df_tweets, 80)\n",
    "print(df_tweets.shape)\n",
    "###END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c9465d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dc1c04354414cf7b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Task 04: Remove duplicates and irrelevant data\n",
    "\n",
    "    a. Identify duplicates: \n",
    "        \n",
    "        Two tweets are indentical if they have same \n",
    "        full_text, created_at, reply_count, favorites_count\n",
    "        \n",
    "        Two different tweets can have the same full_text as other people can re-tweet\n",
    "        \n",
    "        Note: trying to combine the above features may be expensive\n",
    "    \n",
    "    b. remove duplicates\n",
    "\n",
    "Expectation: 1277 duplicates are deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765d0bba",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a3336f0940aedd7b",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "###BEGIN SOLUTION\n",
    "num_rows = df_tweets.shape[0]\n",
    "df_tweets = df_tweets.drop_duplicates(subset = ['full_text', 'reply_count', 'favorite_count','created_at'])\n",
    "print(\"#duplicates: \", num_rows - df_tweets.shape[0])\n",
    "###END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf8ee99",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-71879c841157d7b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Task 05: Clean text\n",
    "\n",
    "Do the following sub tasks for all values in each text columns:\n",
    "+ lowcase\n",
    "+ strip spaces (leading and trailing)\n",
    "\n",
    "Call the functions you have done in the tasks 06 and 07 of the previous assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f73707",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-df51ed24786bb704",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "###BEGIN SOLUTION\n",
    "\n",
    "def lower_text(df):\n",
    "    df1 = df.select_dtypes(include='object')\n",
    "    print(df1.dtypes)\n",
    "    df1 = df1.applymap(lambda x: x.lower() if isinstance(x, str) else \"\")\n",
    "    df_cp = df.copy()\n",
    "    for c in df1.columns:\n",
    "        df_cp[c] = df1[c]\n",
    "    return df_cp\n",
    "\n",
    "\n",
    "def strip_text(df):\n",
    "    df1 = df.select_dtypes(include='object')\n",
    "    df1 = df1.applymap(lambda x: x.strip() if isinstance(x, str) else \"\")\n",
    "    df_cp = df.copy()\n",
    "    for c in df1.columns:\n",
    "        df_cp[c] = df1[c]\n",
    "    return df_cp\n",
    "\n",
    "df_tweets = lower_text(df_tweets)\n",
    "df_tweets = strip_text(df_tweets)\n",
    "\n",
    "df_tweets.head()\n",
    "###END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cc6acd",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5915a3db46ac7d43",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Task 06: Clean tweets\n",
    "\n",
    "Do the following sub tasks for all values in the `full_text` column:\n",
    "+ only keep alphabetical letters\n",
    "+ remove HTML tags and entities\n",
    "+ remove punctuations\n",
    "+ remove stop words (words have no contribution for sentiment identification of sentences)\n",
    "+ stemming words: replace a word with its original version since they have the same meaning and sentiment in a sentence. For example, `happiness` is derived from `happy` => we will replace `happiness` by `happy`\n",
    "\n",
    "You are provided all the functions in the setting up cell. Call them for this task.\n",
    "\n",
    "Note this task will take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a28128",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c2b55833b91ef73d",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "###BEGIN SOLUTION\n",
    "df_tweets['full_text'] = df_tweets['full_text'].apply(keepAlpha)\n",
    "df_tweets['full_text'] = df_tweets['full_text'].apply(cleanHtml)\n",
    "df_tweets['full_text'] = df_tweets['full_text'].apply(cleanPunc)\n",
    "df_tweets['full_text'] = df_tweets['full_text'].apply(removeStopWords)\n",
    "df_tweets['full_text'] = df_tweets['full_text'].apply(stemming)\n",
    "\n",
    "df_tweets.head()\n",
    "###END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbf5ff3",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d2c710329ea735f9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Task 07: Add a new column\n",
    "\n",
    "Add a new column, called `state` and fill values in this column using the the following instructions.\n",
    "\n",
    "We want to derive a state from `user_location`, but the data in `user_location` is very messy: some have cities' names (e.g., Albany), some have cities' names and state, ...\n",
    "\n",
    "So I am including a look-up table file, called `state_full.csv`, here for you. In this file I have 2 columns: `shortState` and `city`. Whenever an user location has a short state name or a city in columns `shortState` or `city`, you will fill the `state` column with the short state.\n",
    "\n",
    "For examples:\n",
    "\n",
    "`user_location` = 'albany, usa' => `state` = 'NY'\n",
    "\n",
    "`user_location` = 'boston, massachusetts' => `state` = 'MA'\n",
    "\n",
    "`user_location` = 'ames, ia' => `state` = 'IA'\n",
    "\n",
    "`user_location` = 'boston' => `state` = 'MA'\n",
    "\n",
    "...\n",
    "Note: this task will take long time (about 3-4 hours) to run\n",
    "\n",
    "Expectation: 53712 short state names (NaN values are not counted) will be filled \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "355b1469",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-88f3c6ec8ed348f2",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m st\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshortState\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[0;32m---> 16\u001b[0m df_tweets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_tweets\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_location\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(get_State)\n\u001b[1;32m     17\u001b[0m num_filled_states \u001b[38;5;241m=\u001b[39m df_tweets\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m df_tweets\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(num_filled_states)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_tweets' is not defined"
     ]
    }
   ],
   "source": [
    "###BEGIN SOLUTION\n",
    "st = pd.read_csv('./state_full.csv')\n",
    "st.shape\n",
    "n = st.shape[0]\n",
    "st['city'] = st['city'].str.lower()\n",
    "st['shortState'] = st['shortState'].str.lower()\n",
    "\n",
    "st.head()\n",
    "\n",
    "def get_State(text):  \n",
    "    for i in range(n):\n",
    "        if (st.iloc[i]['city'] in text) or (st.iloc[i]['shortState'] in text):\n",
    "            return st.iloc[i]['shortState']\n",
    "    return np.nan\n",
    "\n",
    "df_tweets['state'] = df_tweets['user_location'].apply(get_State)\n",
    "num_filled_states = df_tweets.shape[0] - df_tweets.state.isnull().sum()\n",
    "print(num_filled_states)\n",
    "\n",
    "df_tweets.head()\n",
    "df_tweets.to_csv('./clean_tweet_state.csv')\n",
    "\n",
    "###END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fe1c1b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-059b23f932324a02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Task 08: count #tweets and sum up #favorite_count by date\n",
    "\n",
    "Count number of tweets and sum up favorite_count by dates. Store the results in a dataframe, called `df_count_by_date`. Sort the dataframe in the descending order of tweet counts.\n",
    "\n",
    "hint: use `groupby()` on `df_tweets` and then `agg()` with count for full_text and sum for favorite_count\n",
    "\n",
    "expectation: df_count_by_date.shape = (315, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15e4d2a1",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-abd5d6c440c00379",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(315, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>favorite_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created_at</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-10-02</th>\n",
       "      <td>20034</td>\n",
       "      <td>646194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-03</th>\n",
       "      <td>12342</td>\n",
       "      <td>191000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-04</th>\n",
       "      <td>5265</td>\n",
       "      <td>100905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-05</th>\n",
       "      <td>3984</td>\n",
       "      <td>203271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-06</th>\n",
       "      <td>2635</td>\n",
       "      <td>227535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-16</th>\n",
       "      <td>2372</td>\n",
       "      <td>66404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-08</th>\n",
       "      <td>2337</td>\n",
       "      <td>43023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-10</th>\n",
       "      <td>1753</td>\n",
       "      <td>9763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-07</th>\n",
       "      <td>1746</td>\n",
       "      <td>23762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-09</th>\n",
       "      <td>1676</td>\n",
       "      <td>50944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            full_text  favorite_count\n",
       "created_at                           \n",
       "2020-10-02      20034          646194\n",
       "2020-10-03      12342          191000\n",
       "2020-10-04       5265          100905\n",
       "2020-10-05       3984          203271\n",
       "2020-10-06       2635          227535\n",
       "2020-12-16       2372           66404\n",
       "2020-10-08       2337           43023\n",
       "2020-09-10       1753            9763\n",
       "2020-10-07       1746           23762\n",
       "2020-09-09       1676           50944"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###BEGIN SOLUTION\n",
    "df_count_by_date = df_tweets.groupby('created_at').agg({'full_text': 'count', 'favorite_count':'sum'})\n",
    "df_count_by_date.sort_values(['full_text'], inplace = True, ascending = False)\n",
    "print(df_count_by_date.shape)\n",
    "df_count_by_date.head(10)\n",
    "\n",
    "###END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83face4a",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bf4ad2ae2956c051",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Task 09: count #tweets and sum up #favorite_count by state\n",
    "\n",
    "Count number of tweets and sum up favorite_count by states. Store the results in a dataframe, called `df_count_by_state`. Sort the dataframe in the descending order of tweet counts.\n",
    "\n",
    "hint: use `groupby()` on `df_tweets` and then `agg()` with count for full_text and sum for favorite_count\n",
    "\n",
    "expectation: df_count_by_state.shape = (51, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b6fe902",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-69d7ce514f863772",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>favorite_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>10243</td>\n",
       "      <td>1307798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TX</th>\n",
       "      <td>5734</td>\n",
       "      <td>80652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FL</th>\n",
       "      <td>4221</td>\n",
       "      <td>351380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NY</th>\n",
       "      <td>3727</td>\n",
       "      <td>1640301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WA</th>\n",
       "      <td>3298</td>\n",
       "      <td>2557999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       full_text  favorite_count\n",
       "state                           \n",
       "CA         10243         1307798\n",
       "TX          5734           80652\n",
       "FL          4221          351380\n",
       "NY          3727         1640301\n",
       "WA          3298         2557999"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###BEGIN SOLUTION\n",
    "df_count_by_state = df_tweets.groupby('state', dropna=True).agg({'full_text': 'count', 'favorite_count':'sum'})\n",
    "df_count_by_state.sort_values(['full_text'], inplace = True, ascending = False)\n",
    "print(df_count_by_state.shape)\n",
    "df_count_by_state.head()\n",
    "\n",
    "###END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482e252e",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-52d576ec65ad099d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Task 10: Top 10 tweets\n",
    "\n",
    "+ Find tweets in top 10 highest reply_count, ordering from the highest to the least.\n",
    "+ Find tweets in top 10 highest retweet_count, ordering from the highest to the least.\n",
    "+ Find tweets in top 10 highest favorite_count, ordering from the highest to the least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dafa1cff",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-703d87775eb8aeb4",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484      69341\n",
      "15326    29771\n",
      "12924    25817\n",
      "13294    24714\n",
      "3531     22788\n",
      "13399    19417\n",
      "15254    19400\n",
      "5495     17791\n",
      "16840    15948\n",
      "785      14391\n",
      "Name: reply_count, dtype: int64\n",
      "                                               full_text  created_at  \\\n",
      "484    hydroxychloroquin amp azithromycin taken toget...  2020-03-21   \n",
      "486    pleas take hydroxychloroquin plaquenil plus az...  2020-03-21   \n",
      "93441  trump kept tell us take hydroxychloroquin inje...  2020-10-06   \n",
      "540    pleas spread hydroxychloroquin amp azithromyci...  2020-03-21   \n",
      "16840  pleas watch high respect dr harvey risch yale ...  2020-08-24   \n",
      "9323                hydroxychloroquin https co ymobdcfgx  2020-05-19   \n",
      "13294  high respect henri ford health system report b...  2020-07-07   \n",
      "12905  want ensur everyon understand graviti situat h...  2020-07-03   \n",
      "13030       hydroxychloroquin work work whole time tweet  2020-07-04   \n",
      "14600  imagin child porn taken social media quick hyd...  2020-07-29   \n",
      "\n",
      "                     user_location  friends_count  followers_count  \\\n",
      "484                 washington, dc             50         85725414   \n",
      "486    republic of the philippines            335            27605   \n",
      "93441          eugene@coolquit.com           6154           524500   \n",
      "540                   new york, ny             48             6703   \n",
      "16840               washington, dc             50         85725414   \n",
      "9323                  brooklyn, ny           3005          2276258   \n",
      "13294               washington, dc             50         85725414   \n",
      "12905          manhattan, new york           4636           321973   \n",
      "13030          manhattan, new york           4636           321973   \n",
      "14600              los angeles, ca            327            11610   \n",
      "\n",
      "       reply_count  retweet_count  favorite_count  is_with_url state  \n",
      "484          69341         101604          374415            0    WA  \n",
      "486           2501          69684          139878            0   NaN  \n",
      "93441         2558          63176          203950            0    OR  \n",
      "540           1433          53780           89525            0    NY  \n",
      "16840        15948          53778          124556            0    WA  \n",
      "9323          5664          50913          206341            0    NY  \n",
      "13294        24714          48914          163587            0    WA  \n",
      "12905         2688          48780           85632            0    NY  \n",
      "13030         1670          41374          107698            0    NY  \n",
      "14600         1997          38429          123763            0    CA  \n",
      "                                               full_text  created_at  \\\n",
      "484    hydroxychloroquin amp azithromycin taken toget...  2020-03-21   \n",
      "9323                hydroxychloroquin https co ymobdcfgx  2020-05-19   \n",
      "93441  trump kept tell us take hydroxychloroquin inje...  2020-10-06   \n",
      "13294  high respect henri ford health system report b...  2020-07-07   \n",
      "52696  trump receiv regeneron polyclon antibodi cockt...  2020-10-02   \n",
      "486    pleas take hydroxychloroquin plaquenil plus az...  2020-03-21   \n",
      "90726  presid receiv multipl medic noteworthi none hy...  2020-10-05   \n",
      "70263              hydroxychloroquin stand back stand by  2020-10-02   \n",
      "16840  pleas watch high respect dr harvey risch yale ...  2020-08-24   \n",
      "14600  imagin child porn taken social media quick hyd...  2020-07-29   \n",
      "\n",
      "                     user_location  friends_count  followers_count  \\\n",
      "484                 washington, dc             50         85725414   \n",
      "9323                  brooklyn, ny           3005          2276258   \n",
      "93441          eugene@coolquit.com           6154           524500   \n",
      "13294               washington, dc             50         85725414   \n",
      "52696          eugene@coolquit.com           6154           524500   \n",
      "486    republic of the philippines            335            27605   \n",
      "90726                                         870          1397401   \n",
      "70263      los angeles, california           6315           115264   \n",
      "16840               washington, dc             50         85725414   \n",
      "14600              los angeles, ca            327            11610   \n",
      "\n",
      "       reply_count  retweet_count  favorite_count  is_with_url state  \n",
      "484          69341         101604          374415            0    WA  \n",
      "9323          5664          50913          206341            0    NY  \n",
      "93441         2558          63176          203950            0    OR  \n",
      "13294        24714          48914          163587            0    WA  \n",
      "52696         1574          37584          153426            0    OR  \n",
      "486           2501          69684          139878            0   NaN  \n",
      "90726         2228          22678          135147            0   NaN  \n",
      "70263          855          18917          126349            0    CA  \n",
      "16840        15948          53778          124556            0    WA  \n",
      "14600         1997          38429          123763            0    CA  \n"
     ]
    }
   ],
   "source": [
    "###BEGIN SOLUTION\n",
    "\n",
    "df_top10_reply = df_tweets.sort_values(['reply_count'], ascending = False).iloc[:10]\n",
    "df_top10_retweet = df_tweets.sort_values(['retweet_count'], ascending = False).iloc[:10]\n",
    "df_top10_favorite = df_tweets.sort_values(['favorite_count'], ascending = False).iloc[:10]\n",
    "\n",
    "print(df_top10_reply['reply_count'])\n",
    "print(df_top10_retweet['retweet_count'])\n",
    "print(df_top10_favorite['favorite_count'])\n",
    "#df_top10_reply.head()\n",
    "\n",
    "###END SOLUTION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
